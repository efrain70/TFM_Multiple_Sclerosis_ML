Teniendo presente el objetivo principal de este trabajo, la investigación se divide en tres partes principales: los datos, los algoritmos y los resultados obtenidos.

Respecto a los datos disponibles para la investigación, se ha estudiado en detalle el origen de éstos, cómo son calculados y el significado de cada uno de ellos. También fue necesario estudiar cómo están presentados, la distribución de los atributos y su calidad para su integración en el sistema de aprendizaje automático. 

La metodología seguida durante la selección de algoritmos para la obtención de los modelos sigue un carácter iterativo sobre los algoritmos de aprendizaje automático. Empezando por los algoritmos obtenidos durante la fase del estudio del arte e incorporándose a posteriori nuevos algoritmos, se han aplicado los datos y optimizados los hiperparámetros que configuran a cada uno de ellos. En cada iteración se han probado fases nuevas para el preprocesado de los datos y nuevas configuraciones para la ejecución de modo que los resultados se han ido obteniendo formaran parte de la heurística aprendida para retomar nuevamente el proceso iterativo.

Finalmente, se han expuesto en un informe autogenerado durante el proceso experimentos con sus configuraciones usadas, el proceso de obtención de los modelos y resultados obtenidos durante la ejecución. 

Como herramienta principal se ha usado el lenguaje de programación interpretado Python para la implementación de software necesario para la obtención y adecuación datos proveídos. Todo el trabajo realizado para la ejecución y selección de los algoritmos junto con sus hiperparámetros y elementos de preprocesado se ha realizado dentro del framework Scikit-learn \cite{Scikit-learn:Documentation} y con la ayuda de otros frameworks y librerías dedicados al aprendizaje automático; Keras \cite{KerasDocumentation}, TensorFlow \cite{TensorFlow}, Caffe2 \cite{Caffe2Framework} y, enfocado a la neurología, ann4brains \cite{Kawahara2017BrainNetCNN:Neurodevelopment}. También fue necesario el uso de las librerías científicas para el tratamiento de datos; Numpy \cite{NumPyNumPy}, SciPy \cite{SciPy.orgSciPy.org} y Pandas \cite{PythonLibrary}. Con ayuda de librerías gráficas de Python como seaborn \cite{Seaborn:Documentation} y matplotlib \cite{InstallationDocumentation} se generan las gráficas y representaciones visuales de los datos.

La documentación de las ejecuciones se exponen usando notebooks de Jupyter \cite{ProjectHome} donde se presentan los resultados y ejemplos prácticos del estudio. Como plataforma de edición del contenido se ha usado Google Drive y  \LaTeX \cite{LaTeXSystem}, dentro de la plataforma Overleaf \cite{Overleaf:Preview} como lenguaje de edición para la documentación final. 

Todo el código generado durante la investigación se ha publicado bajo licencia Open Source GPLv3. Para la documentación estática se ha usado la licencia de Reconocimiento-NoComercial-CompartirIgual 3.0 España de CreativeCommons (CC BY-NC-SA 3.0 ES). Destacar que los datos proporcionados no se pueden exponer para garantizar su privacidad y confidencialidad. 
